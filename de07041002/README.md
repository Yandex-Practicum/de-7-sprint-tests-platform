# Комментарий от тестописателя
На платформе невозможно реализовать Hadoop-кластер и HDFS — чтобы дать студенту возможность писать и проверять код на платформе, мы подготовили файловую структуру похожую на ту, что доступна на кластере.

Важные отличия:

1. Папки для чтения лежат в каталоге `/home/student` — если на кластере студент читает данные из `/user/master/data/events/date=2022-05-25`, то на платформе это будет `/home/student/user/master/data/events/date=2022-05-25`.

2. Папки для записи лежат в каталоге `/home/student/tmp` — если на кластере студент пишет данные в `/user/USERNAME/analytics/test`, то на платформе это будет `/home/student/tmp/user/USERNAME/analytics/test`.

3. На платформу не получится завести все данные с кластера. Взяли данные за `2022-05-01`, `2022-05-25` и `2022-05-31` в формате JSON из `master` и папки `channels`, `tags_verified` из `snapshots`. Создали каталог `USERNAME` с данными за май 2022-ого партиционированные по `date` и `event_type` в формате Parquet.

4. Имя пользователя на платформе `USERNAME`, а на кластере нужно будет использовать свой логин (выдаёт Telegram-бот).

### ВАЖНО:

Spark по умолчанию не может перезаписывать данные — возникает конфликт с тем как устроена работа прекода в сниппетах.

1) Прекод только в тесте — не работает **Выполнить**, но работает **Проверить**. 

2) Прекод только в JSON-конфиге — работает и **Выполнить**, и **Проверить**, но только если используется `.mode("overwrite")` для операции записи в Spark.

**Note:** Без `.mode("overwrite")`  во время Проверки будет ошибка, что файл уже существует (хотя он создаётся в задаче). Воспроизводится только в сниппетах. Выглядит так,  будто решение студента и, возможно, авторское выполняются дважды, до и во время тестов.

### ТАКЖЕ ВАЖНО
Критически важно, чтобы студент формировал именно такие пути и именно с такими аргументами, так как аргументы командной строки будут жёстко заданы в тесте или прекоде:

`f"{base_input_path}/date={date}"`

`f"{base_output_path}/date={date}"`

На кластере студент может сам решить с какими аргументами запускать свои джобы и соответственно писать скрипты под них, НО не в тренажёре. Важно, чтобы он работал с  аргументами `date`, `base_input_path` и `base_output_path` в том виде, в котором мы определяем их в тесте или прекоде — значит это должно быть отражено в условии.

Например, в условии задачи можно дать пример того, какие аргументы будут передаваться во время запуска скрипта:

`date` — `"2022-05-31"`
`base_input_path` — `"/home/student/user/master/data/events"`
`base_output_path` — `"/home/student/tmp/user/USERNAME/data/events"`

# Путь в админке для тестирования
DE (Аналитик 2.0 Beta) / Организация Data Lake / PySpark для инженера данных / Собираем джобу / Задание 2 (TEST)

# Ссылка на админку для тестирования
https://prestable.admin.praktikum.yandex-team.ru/faculties/d5b98ce5-3d91-47eb-ab9d-4df2bd9f465d/professions/4fdc51de-5615-472e-b31a-3c7ece22b3f0/tracks/88492e2d-9a7c-4123-a412-9aef5e024fe5/courses/f7eb0c28-7528-4c43-b1ce-f2b98b358282/topics/a7082746-1453-4b33-a129-96b3449f8de8/lessons/7fc52da1-720a-48b0-b40c-5fc485b8bdb2/theory/

# Ссылка на ноушин с формулировкой задания
https://www.notion.so/praktikum/10-e73390e02b8c4494af2a28fb2c68f88f?pvs=4#bbf9a30558994a2582be5bbecdca0eb6